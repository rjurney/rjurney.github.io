<html>
<head>
    <!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

<!-- Optional theme -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
</head>
<body>
<p>This is part three of a series of blog posts covering new developments in the Hadoop pantheon that enable productivity throughout the lifecycle of big data.  In a series of posts, weâ€™re exploring the full lifecycle of data in the enterprise: Introducing new data sources to the Hadoop filesystem via ETL, processing this data in data-flows with Pig and Python to expose new and interesting properties, consuming this data as an analyst in Hive, and discovering and accessing these resources as analysts and application developers using HCatalog and Templeton.

</p><ul><li>Series Part One: Avroizing the Enron Emails.  In that post, we used Pig to extract, transform and load a MySQL database of the Enron emails to document format and serialize them in Avro.  The <a href="http://s3.amazonaws.com/rjurney.public/hive-site.xml">Enron emails are available in Avro format here.</a></li>

<li>Series Part Two: Mining Avros with Pig, Consuming Data with Hive.  In part two of the series, we extracted new and interesting properties from our data for consumption by analysts and users, using Pig, EC2 and Hive.  Code examples for this post are available here: <a href="https://github.com/rjurney/enron-hcatalog.">https://github.com/rjurney/enron-hcatalog.</a></li>

<li><a href="http://hortonworks.com/blog/the-data-lifecycle-part-three-booting-hcatalog-on-elastic-mapreduce/">Series Part Three: Booting HCatalog on Elastic MapReduce.</a>  Here we will use HCatalog to streamline the sharing of data between Pig and Hive, and to aid data discovery for consumers of processed data.</li>
</ul>
</body>
</html>